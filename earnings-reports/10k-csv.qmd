# Extracting data from 10k filings

## Motivation


Load outlines

```{python}
import outlines
```

Loading a quantized model

```{python}
language_model = "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
model = outlines.models.transformers(
    language_model,
    device="cuda",
    # model_kwargs={"trust_remote_code": True}
)
```


Load the language model (this takes a while)

```{python}
# language_model = "microsoft/Phi-3.5-mini-instruct"
# # # language_model = "meta-llama/Meta-Llama-3-8B-Instruct"

# model = outlines.models.transformers(
#     language_model,
#     device="cuda",
# )
```

Add a convenience function to convert a user prompt and system prompt to a prompt for the language model. This handles the chat template and tokenization for different models.

```{python}
from transformers import AutoTokenizer

# Load the tokenizer
TOKENIZER = AutoTokenizer.from_pretrained(language_model)
EOS_TOKEN = TOKENIZER.eos_token

def to_prompt(user_prompt="", system_prompt=""):

    chat = []

    if len(system_prompt) > 0:
        chat.append({'role':'system', 'content':system_prompt})

    if len(user_prompt) > 0:
        chat.append({'role':'user', 'content':user_prompt})

    tokenized_chat = TOKENIZER.apply_chat_template(
        chat,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    )

    decoded_chat = TOKENIZER.decode(tokenized_chat[0])
    return decoded_chat

# Example
print(to_prompt(
    user_prompt="What's up?",
    # system_prompt="Say hello"
))
```

Load some data to extract.

```{python}
# Load the data from 10k/nvda-20220130.html
data = open(
    "10k/nvda-20240128.html",
    # "10k/goog-20240630.html",
    # "10k/yahoo.html",
    encoding="latin-1"
).read()
```

Let's test out markdownify.

```{python}
from markdownify import markdownify as md
markdown_document = md(data, strip=['a', 'b', 'i', 'u', 'code', 'pre'])
print(markdown_document)

import tiktoken
enc = tiktoken.get_encoding("o200k_base")
len(enc.encode(markdown_document))
```

`markdown_document` is a bit messy, but it's a good start. Let's cut it into pages. Pages here are separated by horizontal rules. The first line of each page can contain a lot of token-heavy markdown, so we'll strip that out.

```{python}
pages = [page.strip() for page in markdown_document.split("\n---\n")]

# Remove the first line of each page
pages = ["\n".join(page.split("\n")[1:]) for page in pages]

print(f"Loaded {len(pages)} pages")
```

Let's take a look at the first few pages.

```{python}
for i, page in enumerate(pages[0:10], 1):
    print(f"\nPage {i}:")
    print(page)
```

METHOD USING TABLES ONLY

```{python}
from bs4 import BeautifulSoup

def strip_tables_to_text(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    tables = []

    for table in soup.find_all('table'):
        # Initialize title elements list
        title_elements = []

        # Look at current element and traverse up through parents
        current_element = table
        for _ in range(3):  # Check up to 3 levels up
            # Get previous siblings of current element
            prev_element = current_element.find_previous_sibling()
            for _ in range(10):
                if prev_element:
                    # Check if it's a div with text or any element with substantial text
                    text = prev_element.get_text(strip=True)
                    if text and len(text) > 5:  # Avoid empty or very short texts
                        title_elements.insert(0, text)
                    prev_element = prev_element.find_previous_sibling()

            # Move up to parent for next iteration
            if current_element.parent:
                current_element = current_element.parent

        # Rest of the function remains the same
        table_text = []
        if title_elements:
            table_text.extend([f"### {title}" for title in title_elements])
            table_text.append("")  # Add blank line after titles

        rows = []
        for tr in table.find_all('tr'):
            row = [cell.get_text(strip=True) for cell in tr.find_all(['td', 'th'])]
            rows.append('\t'.join(row))
        table_text.append('\n'.join(rows))

        tables.append('\n'.join(table_text))

    return tables

# Usage
text_tables = strip_tables_to_text(data)
for i, table in enumerate(text_tables, 1):
    print(f"\nTable {i}:")
    print(table)
```

Print out a few tables

```{python}
for i, table in enumerate(text_tables[0:20], 1):
    print(f"\nTable {i}:")
    print(table)
```

## Finding the income statement

```{python}
yesno = outlines.generate.choice(model, ["Yes", "No"])

categories = []
for i in range(20):
    print(f"\nPage {i}:")
    prompt = to_prompt(
        user_prompt=f"""
        Analyze the following page from a financial filing and determine if it contains an income statement (also known as statement of operations).

        Page Content:
        {text_tables[i]}

        Criteria for identification:
        1. Must contain key income statement line items like:
        - Revenue/Sales
        - Cost of Revenue/Cost of Sales
        - Operating Expenses
        - Net Income/Loss
        2. Must show financial results for specific time periods
        3. Must be a primary financial statement (not just discussion or analysis)
        4. Numbers should be presented in a structured tabular format

        Answer only 'Yes' if this page contains a complete income statement table, or 'No' if it does not.
        Do not explain your reasoning - just answer 'Yes' or 'No'.
        """,
        # system_prompt="You are an expert accountant that locates relevant financial tables in a 10q filing."
    )
    result = yesno(prompt)
    print(result)
    if result == "Yes":
        categories.append(i)
```

```{python}
print(categories)
```

Look at pages flagged as income statements

```{python}
for i in categories:
    print(f"\nPage {i}:")
    print(text_tables[i])
```

## Set up the extractor

Here we can build a function to create a regex pattern to extract the CSV data. This can use arbitrary columns and data types.

```{python}
from typing import List

def create_regex_pattern(columns: List[str], data_types: List[str]) -> str:
    # Define regex patterns for common data types
    type_patterns = {
        "string": r"([a-zA-Z\s]+?)",
        "ticker": r"([A-Z]{,8})",
        "year": r"(\d{4})",
        "quarter": r"(Q[1-4])",
        "number": r"(-?\d+(?:\.\d{1,2})?)",
        "revenue": r"(-?\d{1,3}(?:\.\d{1,2})?[BM]?)",
        "percentage": r"(-?\d{1,3}(?:\.\d{1,2})?%?|null)",
        "nullable_number": r"(-?\d+(?:\.\d{1,2})?|null)"
    }

    # Create the header line
    header = ",".join(columns)

    # Create the data capture patterns
    data_patterns = [type_patterns[dtype] for dtype in data_types]
    data_line = ",".join(data_patterns)

    return f"{header}(\n{data_line})*?(\n\n)" # terminate with a newline
```

```{python}
import re

# columns = ["year", "quarter", "revenue", "gross_profit", "operating_income", "net_income"]
# data_types = ["year", "quarter", "number", "number", "number", "number"]

columns = ["year", "quarter", "data_point_name", "data_point_value"]
data_types = ["year", "quarter", "string", "nullable_number"]
csv_pattern = create_regex_pattern(columns, data_types)

csv_pattern = 'year,quarter,data_point_name,data_point_value(\n(\\d{4}),(Q[1-4]),(revenue|gross_profit|operating_income|net_income),(-?\\d+(?:\\.\\d{1,2})?|null))*?(\n\n)'

csv_extractor = outlines.generate.regex(
    model,
    csv_pattern,
    sampler=outlines.samplers.beam_search()
    # sampler=outlines.samplers.multinomial()
)
```

Extract data from the income statement candidates

```{python}
relevant_pages = "\n\n".join([text_tables[i] for i in categories])

# Add the first page to the prompt
relevant_pages = pages[0] + "\n\n" + relevant_pages

prompt = to_prompt(
    user_prompt=f"""
    Extract quarterly (3-month) financial data from this set of pages. Pages
    are from a 10k filing and were chosen because they may contain an income statement.

    Extract a year, quarter, data point name, and data point value for each row. Valid data point names
    are revenue, gross profit, operating income, and net income.

    # Relevant pages:

    {relevant_pages}

    # Key instructions:
    1. Use only quarterly (3-month) periods, often labeled as "Q1" or "3 months ended..."
    2. Do not include annual or year-to-date figures
    3. Use NULL for missing values
    4. Each row should contain data for a single quarter, if available

    # Output format:

    - CSV format with headers: {','.join(columns)}
    - Use only quarterly (3-month) periods
    - Skip annual or year-to-date figures
    - Use NULL for missing values
    - Numbers should be in millions/billions as presented
    - If no data are found, do not create a row.
    - Enter two newline characters to terminate the CSV when no more data are found.

    # Notes:
    - Financial statements often present data spanning different periods, such as year-to-date or 9-month data. Be sure to focus on quarterly or 3-month data.
    """,
    # system_prompt="You extract data from 10k filings and output it in CSV format."
)

# Count the tokens
enc = tiktoken.get_encoding("o200k_base")
len(enc.encode(prompt))

print(csv_extractor(prompt, max_tokens=500))

# Simple text generation instead
# generator = outlines.generate.text(model)
# print(generator(to_prompt(user_prompt=f"""
# Please list the company's revenue for each quarter.

# {relevant_pages}
# """), max_tokens=100))
```

```{python}
print(csv_extractor(prompt, max_tokens=1000))
```
